---
title: "Boosted Trees"
author: "Alex Wood, Wyatt Lansford, Luke Kim, and Mitchell Neat"
date: "12/3/2019"
output: beamer_presentation
header-includes:
  - \usepackage{multicol}
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



## Outline

- Boosted Trees

- Gradient Boosted Trees
  - Parameters
  - Math/Algorithm Details
  - Visual Examples
  
- Boston Crime Data
  - Regression
  - Trees
  - Gradient Boosted Trees

- Summary

## What are Decision Trees?


## What is Boosting?

- Combining weak learners (generally trees) into a strong classifier

- Key Aspects:
  - weighting data
  - misclassification
  - regularization

## Combined: Boosted Decision Trees

- Iteratively adding learners (trees) to minimize the loss function of the decision tree.

[[[ Loss Function ]]]

[[[ Regularization Function ]]]

- Regularizes after each iteration???


## Decision Tree vs. Boosted Decision Tree

\begin{multicols}{2}

  \null \vfill
  \null \vfill
  \includegraphics[width=.5\textwidth]{tree.png}
  \vfill \null
  \vfill \null
\columnbreak
  \null \vfill
  \includegraphics[width=.5\textwidth]{boosted_tree.png}
  \vfill \null
\end{multicols}


## Types of Boosted Trees

- ADA Boost

  - One of the original boosted tree methods
  
- Gradient Boosting

  - Uses gradient descent to optimize decision tree

- XGBoost

  - Specific subtype of gradient boosted trees
  - "eXtreme Gradient Boosting"

## Generic Tree Parameters
\begin{multicols}{2}
  \begin{itemize}
    \item  Maximum depth
    \item  Maximum features
    \item  Maximum features
    \item  Minimum samples per leaf
  \end{itemize}
  \columnbreak

  \null \vfill
  \includegraphics[width=.5\textwidth]{tree_diagram.png}
  \vfill \null
\end{multicols}

## GBT Parameters

- Loss Function

- Learning Rate

- Subsample Size

- Number of Trees

  - Generic tree parameters apply here

## Gradient Boosted Tree





  \null \vfill
  \begin{itemize}
    \item  Take set of features and split data recursively based on features.
    \item  Splits are chosen to minimize entropy.
  \end{itemize}
  \vfill \null
  

\begin{center}
  \null \vfill
  \includegraphics[width=.9\textwidth]{gradient_boosted_tree.png}
  \vfill \null
\end{center}

## Mathematical Background

- Loss is the difference between the true and predicted

[[[ Loss Function ]]]

- Regularization helps combat complexity and overfitting

[[[ Regularization Function ]]]

- Combining these helps the supervised learning aspect of GBT, providing the best and simplest fit.

## GBT Algorithm

## Step 2 in Detail

## Detailed Algorithm
\begin{center}
\includegraphics[width=.8\textwidth]{algorithm.png}
\end{center}
## Visual Examples

## Why?

## Introducing the Dataset

## Exploratory Plots

## Regression Attempts

## Regular Tree Fit

## GBT Model

## GBT Plots

## Differences

## Optimizing GBT Model

## Differences

## Summary

The Boosted Tree method is a powerful machine learning algorithm used to improve prediction results from a decision tree. 

The general idea is to "compute a sequence of decision trees in which each successive tree is built for the prediction results of the preceeding tree."

## Summary

