---
title: "Boosted Trees"
author: "Alex Wood, Wyatt Lansford, Luke Kim, and Mitchell Neat"
date: "12/3/2019"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Outline

- Boosted Trees

- Gradient Boosted Trees
  - Parameters
  - Math/Algorithm Details
  - Visual Examples
  
- Bostom Crime Data
  - Regression
  - Trees
  - Gradient Boosted Trees

- Summary

## What are Decision Trees?

- Take set of features and split data recursively based on features.

- Splits are chosen to minimize entropy.

[[[Tree Picture]]]

## What is Boosting?

- Combining weak learners (trees) into strong classifiers.

- Key Aspects:
  - weighting data
  - misclassification
  - regularization

## Combined: Boosted Decision Trees

- Iteratively adding learners (trees) to minimize the loss function of the decision tree.

[[[ Loss Function ]]]

[[[ Regularization Function ]]]

- Regularizes after each iteration???

## Types of Boosted Trees
```{r pic}
knitr::include_graphics('/home/wlans4/Pictures/Screenshot from 2019-11-24 00-45-31.png.jpg')
```
- ADA Boost

  - One of the original boosted tree methods
  
- Gradient Boosting

  - Uses gradient descent to optimize decision tree

- XGBoost

  - Specific subtype of gradient boosted trees
  - "eXtreme Gradient Boosting"

## Generic Tree Parameters

- Maximum depth

- Maximum features

- Minimum samples per leaf

[[[labeled graphic]]]

## GBT Parameters

- Loss Function

- Learning Rate

- Subsample Size

- Number of Trees

  - Generic tree parameters apply here

## Gradient Boosted Tree

[[[ GBT Graphic ]]]

## Mathematical Background

- Loss is the difference between the true and predicted

[[[ Loss Function ]]]

- Regularization helps combat complexity and overfitting

[[[ Regularization Function ]]]

- Combining these helps the supervised learning aspect of GBT, providing the best and simplest fit.

## GBT Algorithm

## Step 2 in Detail

## Detailed Algorithm

## Visual Examples

## Why?

## Introducing the Dataset

## Exploratory Plots

## Regression Attempts

## Regular Tree Fit

## GBT Model

## GBT Plots

## Differences

## Optimizing GBT Model

## Differences

## Summary

The Boosted Tree method is a powerful machine learning algorithm used to improve prediction results from a decision tree. 

The general idea is to "compute a sequence of decision trees in which each successive tree is built for the prediction results of the preceeding tree."

## Summary

